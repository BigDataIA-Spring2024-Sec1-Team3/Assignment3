{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaDataPDFClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>language</th>\n",
       "      <th>version</th>\n",
       "      <th>encoding</th>\n",
       "      <th>file_size</th>\n",
       "      <th>s3_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grobid_RR_2024_l3_combined.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>32602</td>\n",
       "      <td>https://bigdata-group3-assignment2.s3.amazonaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grobid_RR_2024_l1_combined.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>52255</td>\n",
       "      <td>https://bigdata-group3-assignment2.s3.amazonaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grobid_RR_2024_l2_combined.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>50250</td>\n",
       "      <td>https://bigdata-group3-assignment2.s3.amazonaw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name language  version encoding  file_size  \\\n",
       "0  Grobid_RR_2024_l3_combined.txt       en      1.0    UTF-8      32602   \n",
       "1  Grobid_RR_2024_l1_combined.txt       en      1.0    UTF-8      52255   \n",
       "2  Grobid_RR_2024_l2_combined.txt       en      1.0    UTF-8      50250   \n",
       "\n",
       "                                              s3_url  \n",
       "0  https://bigdata-group3-assignment2.s3.amazonaw...  \n",
       "1  https://bigdata-group3-assignment2.s3.amazonaw...  \n",
       "2  https://bigdata-group3-assignment2.s3.amazonaw...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file to check its headers and a few rows\n",
    "file_path = '../input-data/metadata-grobid.csv'\n",
    "try:\n",
    "    # Attempt to read the CSV with a specified delimiter (e.g., tab)\n",
    "    data = pd.read_csv(file_path)\n",
    "except Exception as e:\n",
    "    print(\"Error reading the CSV file:\", e)\n",
    "\n",
    "# Display the first few rows to understand the schema\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_86562/927597858.py:14: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('language')\n",
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_86562/927597858.py:21: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('version')\n",
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_86562/927597858.py:27: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('file_size')\n",
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_86562/927597858.py:33: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('file_name')\n",
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_86562/927597858.py:52: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('s3_url', pre=True)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, HttpUrl, Field, validator, constr, ValidationError\n",
    "import re\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class MetaDataPDFClass(BaseModel):\n",
    "    file_name: str\n",
    "    language: str\n",
    "    version: float\n",
    "    encoding: str\n",
    "    file_size: int\n",
    "    s3_url: HttpUrl\n",
    "\n",
    "    @validator('language')\n",
    "    def language_must_be_valid(cls, v):\n",
    "        allowed_languages = ['en']  # Extend this list based on your requirements\n",
    "        if v not in allowed_languages:\n",
    "            raise ValueError(f'language must be one of {allowed_languages}')\n",
    "        return v\n",
    "\n",
    "    @validator('version')\n",
    "    def version_must_be_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError('version must be positive')\n",
    "        return v\n",
    "\n",
    "    @validator('file_size')\n",
    "    def file_size_must_be_non_negative(cls, v):\n",
    "        if v < 0:\n",
    "            raise ValueError('file_size must be non-negative')\n",
    "        return v\n",
    "\n",
    "    @validator('file_name')\n",
    "    def validate_file_name(cls, v):\n",
    "        # Pattern to extract year and level_number from file_name\n",
    "        pattern = r\"Grobid_RR_(?P<year>\\d{4})_l(?P<level_number>\\d+)_combined\\.txt\"\n",
    "        match = re.match(pattern, v)\n",
    "        if not match:\n",
    "            raise ValueError(f\"file_name does not match required pattern: {v}\")\n",
    "\n",
    "        # Extract year and level_number from the match object\n",
    "        year = int(match.group('year'))\n",
    "        level_number = int(match.group('level_number'))\n",
    "\n",
    "        # Perform any additional validation on extracted values if necessary\n",
    "        # For example, ensuring year and level_number fall within expected ranges\n",
    "        if year != 2024 or level_number not in [1, 2, 3]:\n",
    "            raise ValueError(\"file_name contains invalid year or level_number\")\n",
    "\n",
    "        return v\n",
    "\n",
    "    @validator('s3_url', pre=True)\n",
    "    def validate_s3_url(cls, v, values):\n",
    "        if 'file_name' not in values:\n",
    "            raise ValueError(\"file_name must be validated before s3_url\")\n",
    "    \n",
    "        file_name = values['file_name'].strip()  # Ensure no trailing spaces\n",
    "        expected_url_start = f\"https://bigdata-group3-assignment2.s3.amazonaws.com/Grobid/{file_name}\"\n",
    "    \n",
    "        # Check if the actual URL starts with the expected URL start\n",
    "        if not v.startswith(expected_url_start):\n",
    "            raise ValueError(f\"s3_url does not match expected pattern: {v}\")\n",
    "\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pydantic import ValidationError\n",
    "\n",
    "def validate_csv(file_path, model):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        # Assuming the delimiter is a comma, adjust if necessary\n",
    "        reader = csv.DictReader(csvfile)\n",
    "\n",
    "        valid_rows = []\n",
    "        errors = []\n",
    "\n",
    "        for row_number, row in enumerate(reader, start=1):\n",
    "            try:\n",
    "                # Convert the row to the Pydantic model\n",
    "                model_instance = model(**row)\n",
    "                valid_rows.append(model_instance)\n",
    "            except ValidationError as e:\n",
    "                errors.append({'row': row_number, 'error': str(e)})\n",
    "\n",
    "        return valid_rows, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Rows Count: 3\n",
      "No Validation Errors Found.\n"
     ]
    }
   ],
   "source": [
    "# Adjust the file path as necessary\n",
    "valid_data, validation_errors = validate_csv(file_path, MetaDataPDFClass)\n",
    "\n",
    "print(f\"Valid Rows Count: {len(valid_data)}\")\n",
    "if validation_errors:\n",
    "    print(\"Errors Found:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"Row {error['row']}: {error['error']}\")\n",
    "else:\n",
    "    print(\"No Validation Errors Found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContentPDFClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_47678/2830283782.py:17: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(tei, 'lxml')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of XML files to process\n",
    "xml_files = [\n",
    "    '../input-data/Grobid_RR_2024_l1_combined.xml',\n",
    "    '../input-data/Grobid_RR_2024_l2_combined.xml',\n",
    "    '../input-data/Grobid_RR_2024_l3_combined.xml'\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_data = pd.DataFrame(\n",
    "    columns=[\"Title\", \"Idno\", \"Abstract\", \"Chapter_Name\", \"Learning_Outcomes\"])\n",
    "\n",
    "for tei_doc in xml_files:\n",
    "    with open(tei_doc, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "\n",
    "    # Extract the additional fields\n",
    "    doc_title = soup.title.getText() if soup.title else \"N/A\"\n",
    "    idno = soup.find('idno', type='MD5').getText(\n",
    "    ) if soup.find('idno', type='MD5') else \"N/A\"\n",
    "    abstract = soup.abstract.getText() if soup.abstract else \"N/A\"\n",
    "\n",
    "    # Assuming divs_text is already populated as per your code snippet\n",
    "    divs_text = []\n",
    "    for div in soup.body.find_all(\"div\"):\n",
    "        if not div.get(\"type\"):\n",
    "        # Use '\\n' to preserve line breaks\n",
    "            div_text = div.get_text(separator='\\n', strip=True)\n",
    "            divs_text.append(div_text)\n",
    "\n",
    "    plain_text = \"\\n\".join(divs_text)\n",
    "    parts = plain_text.split(\"LEARNING OUTCOMES\")\n",
    "\n",
    "    data = []  # To store tuples of (title, content)\n",
    "\n",
    "    for i in range(1, len(parts)):\n",
    "        # For each part, find the last newline in the previous part to isolate the title\n",
    "        prev_part = parts[i-1].rstrip()\n",
    "        # Find the last occurrence of newline\n",
    "        title_line_index = prev_part.rfind('\\n')\n",
    "        title = prev_part[title_line_index +\n",
    "                        1:] if title_line_index != -1 else prev_part\n",
    "        content = parts[i].strip()\n",
    "        data.append((doc_title, idno, abstract, title, content))\n",
    "\n",
    "    # Creating a temporary DataFrame for the current XML file\n",
    "    df = pd.DataFrame(\n",
    "        data, columns=[\"Title\", \"Idno\", \"Abstract\", \"Chapter_Name\", \"Learning_Outcomes\"])\n",
    "\n",
    "    # Append the data from this file to the main DataFrame\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to CSV\n",
    "all_data.to_csv('ContentPDFClass.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/t0b63cp179v0bgf_vxmy6brh0000gp/T/ipykernel_47678/2683677914.py:14: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Learning_Outcomes')\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, HttpUrl, Field, validator, constr, ValidationError\n",
    "import re\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class ContentPDFClass(BaseModel):\n",
    "    Title: Optional[str] = None\n",
    "    Idno: str = Field(..., pattern=\"^[a-fA-F0-9]{32}$\")\n",
    "    Abstract: str\n",
    "    Chapter_Name: str\n",
    "    Learning_Outcomes: str\n",
    "\n",
    "   # Ensure LearningOutcome contains a specific pattern\n",
    "    @validator('Learning_Outcomes')\n",
    "    def learning_outcome_must_contain_pattern(cls, v):\n",
    "        required_phrase = \"The candidate should be able to:\"\n",
    "        if required_phrase not in v:\n",
    "            raise ValueError(\n",
    "                f\"LearningOutcome must contain '{required_phrase}'\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pydantic import ValidationError\n",
    "\n",
    "def validate_csv(file_path, model):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        # Assuming the delimiter is a comma, adjust if necessary\n",
    "        reader = csv.DictReader(csvfile)\n",
    "\n",
    "        valid_rows = []\n",
    "        errors = []\n",
    "\n",
    "        for row_number, row in enumerate(reader, start=1):\n",
    "            try:\n",
    "                # Convert the row to the Pydantic model\n",
    "                model_instance = model(**row)\n",
    "                valid_rows.append(model_instance)\n",
    "            except ValidationError as e:\n",
    "                errors.append({'row': row_number, 'error': str(e)})\n",
    "\n",
    "        return valid_rows, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Rows Count: 24\n",
      "No Validation Errors Found.\n"
     ]
    }
   ],
   "source": [
    "# Adjust the file path as necessary\n",
    "file_path = 'ContentPDFClass.csv'\n",
    "valid_data, validation_errors = validate_csv(file_path, ContentPDFClass)\n",
    "\n",
    "print(f\"Valid Rows Count: {len(valid_data)}\")\n",
    "if validation_errors:\n",
    "    print(\"Errors Found:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"Row {error['row']}: {error['error']}\")\n",
    "else:\n",
    "    print(\"No Validation Errors Found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
